{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "-  ImageNet\n",
    "    - data수 : 1.2M / 50K\n",
    "    - class : 1000\n",
    "    - class당 data : 1200 개 (train set)\n",
    "- stanford_dogs\n",
    "    - data수 : 1.2K / 8580\n",
    "    - class : 120\n",
    "    - class당 data : 100개 (train set)\n",
    "- CIFAR-100\n",
    "    - data수 : 50K / 10K\n",
    "    - class : 100 (superclass 존재함)\n",
    "    - class당 data : 500개 (train set)\n",
    "- CIFAR-10\n",
    "    - data수 : 50K / 10K\n",
    "    - class : 10\n",
    "    - class당 data : 5000개 (train set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __CIFAR-100이 ImageNet과 stanford_dogs의 class당 data수가 중간쯤 되니 이거로 쌩학습 ㄱㄱ__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/aiffel/KangTae123/aiffel/aug_test_cifar.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cutmix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_box(image_a, image_b, proba = 1.0):\n",
    "    # image.shape = (height, width, channel)\n",
    "    image_size_x = image_a.shape[1]\n",
    "    image_size_y = image_a.shape[0]\n",
    "    \n",
    "    # get center of box\n",
    "    x = tf.cast( tf.random.uniform([],0, image_size_x),tf.int32)\n",
    "    y = tf.cast( tf.random.uniform([],0, image_size_y),tf.int32)\n",
    "    \n",
    "    P = tf.cast( tf.random.uniform([],0,1)<=proba, tf.int32)\n",
    "    \n",
    "    # get width, height of box\n",
    "    width = tf.cast(image_size_x * tf.math.sqrt(1-tf.random.uniform([],0,1)),tf.int32) * P\n",
    "    height = tf.cast(image_size_y * tf.math.sqrt(1-tf.random.uniform([],0,1)),tf.int32) * P\n",
    "    \n",
    "    # clip box in image and get minmax bbox\n",
    "    xa = tf.math.maximum(0, x-width//2)\n",
    "    ya = tf.math.maximum(0, y-height//2)\n",
    "    xb = tf.math.minimum(image_size_x, x+width//2)\n",
    "    yb = tf.math.minimum(image_size_y, y+width//2)\n",
    "    \n",
    "    return xa, ya, xb, yb\n",
    "\n",
    "# mix two images\n",
    "def mix_2_images(image_a, image_b, xa, ya, xb, yb):\n",
    "    image_size_x = image_a.shape[1]\n",
    "    image_size_y = image_a.shape[0] \n",
    "    one = image_a[ya:yb,0:xa,:]\n",
    "    two = image_b[ya:yb,xa:xb,:]\n",
    "    three = image_a[ya:yb,xb:image_size_x,:]\n",
    "    middle = tf.concat([one,two,three],axis=1)\n",
    "    \n",
    "    top = image_a[0:ya,:,:]\n",
    "    bottom = image_a[yb:image_size_y,:,:]\n",
    "    mixed_img = tf.concat([top, middle, bottom],axis=0)\n",
    "    return mixed_img\n",
    "\n",
    "def mix_2_label(image_a, image_b, label_a, label_b, xa, ya, xb, yb, num_classes=100):\n",
    "    image_size_x = image_a.shape[1]\n",
    "    image_size_y = image_a.shape[0] \n",
    "    mixed_area = (xb-xa)*(yb-ya)\n",
    "    total_area = image_size_x*image_size_y\n",
    "    a = tf.cast(mixed_area/total_area, tf.float32)\n",
    "\n",
    "    if len(label_a.shape)==0:\n",
    "        label_a = tf.one_hot(label_a, num_classes)\n",
    "    if len(label_b.shape)==0:\n",
    "        label_b = tf.one_hot(label_b, num_classes)\n",
    "    mixed_label = a*label_a + (1-a)*label_b\n",
    "    return mixed_label\n",
    "\n",
    "def cutmix(image, label, prob = 0.666, batch_size=BATCH_SIZE, img_size=32, num_classes=100, proba = 1.0):\n",
    "    mixed_imgs = []\n",
    "    mixed_labels = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        image_a = image[i]\n",
    "        label_a = label[i]\n",
    "        j = tf.cast(tf.random.uniform([],0, batch_size),tf.int32)\n",
    "        image_b = image[j]\n",
    "        label_b = label[j]\n",
    "        \n",
    "        xa, ya, xb, yb = get_clip_box(image_a, image_b, proba)\n",
    "        mixed_imgs.append(mix_2_images(image_a, image_b, xa, ya, xb, yb))\n",
    "        mixed_labels.append(mix_2_label(image_a, image_b, label_a, label_b, xa, ya, xb, yb))\n",
    "\n",
    "    mixed_imgs = tf.reshape(tf.stack(mixed_imgs),(batch_size, img_size, img_size, 3))\n",
    "    mixed_labels = tf.reshape(tf.stack(mixed_labels),(batch_size, num_classes))\n",
    "    return mixed_imgs, mixed_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MixUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for mixup\n",
    "def mixup_2_images(image_a, image_b, label_a, label_b):\n",
    "    a = tf.random.uniform([],0,1)\n",
    "    \n",
    "    if len(label_a.shape)==0:\n",
    "        label_a = tf.one_hot(label_a, num_classes)\n",
    "    if len(label_b.shape)==0:\n",
    "        label_b = tf.one_hot(label_b, num_classes)\n",
    "    mixed_image= (1-a)*image_a + a*image_b\n",
    "    mixed_label = (1-a)*label_a + a*label_b\n",
    "    \n",
    "    return mixed_image, mixed_label\n",
    "\n",
    "def mixup(image, label, prob = 1.0, batch_size=BATCH_SIZE, img_size=32, num_classes=100):\n",
    "    mixed_imgs = []\n",
    "    mixed_labels = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        image_a = image[i]\n",
    "        label_a = label[i]\n",
    "        j = tf.cast(tf.random.uniform([],0, batch_size),tf.int32)\n",
    "        image_b = image[j]\n",
    "        label_b = label[j]\n",
    "        mixed_img, mixed_label = mixup_2_images(image_a, image_b, label_a, label_b)\n",
    "        mixed_imgs.append(mixed_img)\n",
    "        mixed_labels.append(mixed_label)\n",
    "\n",
    "    mixed_imgs = tf.reshape(tf.stack(mixed_imgs),(batch_size, img_size, img_size, 3))\n",
    "    mixed_labels = tf.reshape(tf.stack(mixed_labels),(batch_size, num_classes))\n",
    "    return mixed_imgs, mixed_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset 전처리\n",
    "\n",
    "    \n",
    "- official code의 norm 적용 (https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/applications/imagenet_utils.py#L158)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "# def normalize_and_resize_img_origin(image, label):\n",
    "#     \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "#     image = tf.image.resize(image, [224, 224])\n",
    "#     return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "# official code normalize (imagenet & cifar-100)\n",
    "def normalize_and_resize_img(image, label, mean=[125.3, 123.0, 113.9], std=[63.0, 62.1, 66.7]):\n",
    "    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "    image = tf.image.resize(image, [32, 32])\n",
    "    R = tf.divide(tf.subtract(image[..., 0], mean[0]), std[0])\n",
    "    G = tf.divide(tf.subtract(image[..., 1], mean[1]), std[1])\n",
    "    B = tf.divide(tf.subtract(image[..., 2], mean[2]), std[2])\n",
    "    \n",
    "    return tf.cast(tf.stack([R,G,B], axis=-1), tf.float32), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_augment(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.central_crop(image, np.random.uniform(0.50, 1.00))\n",
    "    image = tf.image.resize(image, [32, 32])\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- augment 는 이전과같이 baseline으로 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(image, label):\n",
    "    num_classes = 100\n",
    "    onehot_label = tf.one_hot(label, num_classes)\n",
    "    return image, onehot_label\n",
    "\n",
    "def apply_normalize_on_dataset(ds, is_test=False, batch_size=BATCH_SIZE, with_aug=False, with_cutmix=False, with_mixup=False):\n",
    "    ds = ds.map(\n",
    "        normalize_and_resize_img, \n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    ds = ds.batch(batch_size)\n",
    "    \n",
    "    if not is_test and  with_aug:\n",
    "        ds = ds.map(\n",
    "            base_augment,\n",
    "            num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "        )\n",
    "    \n",
    "    if not is_test and with_cutmix:\n",
    "        ds = ds.map(\n",
    "            cutmix,\n",
    "            num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "        )\n",
    "        \n",
    "    elif not is_test and with_mixup:\n",
    "        ds = ds.map(\n",
    "            mixup,\n",
    "            num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "        )\n",
    "    \n",
    "    elif not with_cutmix and not with_mixup and is_test:\n",
    "        ds = ds.map(\n",
    "            onehot,\n",
    "            num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "        )\n",
    "        \n",
    "    if not is_test:\n",
    "        ds = ds.repeat()\n",
    "        ds = ds.shuffle(200)\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'cifar100',\n",
    "    split=['train', 'test'],\n",
    "    as_supervised=True,\n",
    "    shuffle_files=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "num_classes = ds_info.features[\"label\"].num_classes\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_cutmix = apply_normalize_on_dataset(ds_train, with_aug=True, with_cutmix=True, with_mixup=False)\n",
    "ds_train_mixup = apply_normalize_on_dataset(ds_train, with_aug=True, with_cutmix=False, with_mixup=True)\n",
    "\n",
    "ds_test = apply_normalize_on_dataset(ds_test, is_test=True)\n",
    "\n",
    "\n",
    "(ds_train_base, ds_test_base), ds_info = tfds.load(\n",
    "    'cifar100',\n",
    "    split=['train', 'test'],\n",
    "    as_supervised=True,\n",
    "    shuffle_files=True,\n",
    "    with_info=True,\n",
    ")\n",
    "ds_train_base = apply_normalize_on_dataset(ds_train_base, with_aug=True, with_cutmix=False, with_mixup=False)\n",
    "ds_test_base = ds_test_base.map(normalize_and_resize_img, \n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "EPOCH = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basemodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentation 적용 할 모델 (bsseline)\n",
    "\n",
    "resnet50_baseline = keras.models.Sequential([\n",
    "    keras.applications.resnet.ResNet50(\n",
    "        include_top=False,\n",
    "        input_shape=(32, 32,3),\n",
    "        pooling='avg',\n",
    "    ),\n",
    "    keras.layers.Dense(num_classes, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- base는 SGD 모멘텀, decay있는게 3에폭동안 수렴 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(2020)\n",
    "resnet50_baseline.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=['acc']\n",
    ")\n",
    "\n",
    "resnet50_base_hist = resnet50_baseline.fit(\n",
    "    ds_train_base, # augmentation 적용하지 않은 데이터셋 사용\n",
    "    steps_per_epoch=int(ds_info.splits['train'].num_examples/BATCH_SIZE),\n",
    "    validation_steps=int(ds_info.splits['test'].num_examples/BATCH_SIZE),\n",
    "    epochs=EPOCH,\n",
    "    validation_data=ds_test_base,\n",
    "    verbose=1,\n",
    "    use_multiprocessing=True,\n",
    ")\n",
    "\n",
    "resnet50_baseline.save_weights('./cifar100_aug/resnet50_baseline')\n",
    "with open('./cifar100_aug/resnet50_base_hist', 'wb') as file_pi:\n",
    "    pickle.dump(resnet50_base_hist.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_base_hist = pickle.load(open('./cifar100_aug/resnet50_base_hist', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(resnet50_base_hist['loss'], 'r')\n",
    "plt.plot(resnet50_base_hist['val_loss'], 'b')\n",
    "plt.title('baseline loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train_loss', 'val_loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(resnet50_base_hist['sparse_categorical_accuracy'], 'r')\n",
    "plt.plot(resnet50_base_hist['val_sparse_categorical_accuracy'], 'b')\n",
    "plt.title('baseline categoical_accuracy')\n",
    "plt.ylabel('ACC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train_accuracy', 'val_accuracy'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cutmix model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_cutmix = keras.models.Sequential([\n",
    "    keras.applications.resnet.ResNet50(\n",
    "        include_top=False,\n",
    "        input_shape=(32, 32,3),\n",
    "        pooling='avg',\n",
    "    ),\n",
    "    keras.layers.Dense(num_classes, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_cutmix.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=['acc']\n",
    ")\n",
    "\n",
    "resnet50_cutmix_hist = resnet50_cutmix.fit(\n",
    "    ds_train_cutmix, # augmentation 적용하지 않은 데이터셋 사용\n",
    "    steps_per_epoch=int(ds_info.splits['train'].num_examples/BATCH_SIZE),\n",
    "    validation_steps=int(ds_info.splits['test'].num_examples/BATCH_SIZE),\n",
    "    epochs=EPOCH,\n",
    "    validation_data=ds_test,\n",
    "    verbose=1,\n",
    "    use_multiprocessing=True,\n",
    ")\n",
    "\n",
    "resnet50_cutmix.save_weights('./cifar100_aug/resnet50_cutmix')\n",
    "with open('./cifar100_aug/resnet50_cutmix_hist', 'wb') as file_pi:\n",
    "    pickle.dump(resnet50_cutmix_hist.history, file_pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_cutmix_hist = pickle.load(open('./cifar100_aug/resnet50_cutmix_hist', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(resnet50_cutmix_hist['loss'], 'r')\n",
    "plt.plot(resnet50_cutmix_hist['val_loss'], 'b')\n",
    "plt.title('cutmix model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train_loss', 'val_loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(resnet50_cutmix_hist['categorical_accuracy'], 'r')\n",
    "plt.plot(resnet50_cutmix_hist['val_categorical_accuracy'], 'b')\n",
    "plt.title('cutmix model categoical_accuracy')\n",
    "plt.ylabel('ACC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train_accuracy', 'val_accuracy'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(resnet50_cutmix_hist['val_loss'], 'r')\n",
    "plt.plot(resnet50_base_hist['val_loss'], 'b')\n",
    "plt.title('cutmix vs base loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['cutmix loss', 'base loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(resnet50_cutmix_hist['val_categorical_accuracy'], 'r')\n",
    "plt.plot(resnet50_base_hist['val_sparse_categorical_accuracy'], 'b')\n",
    "plt.title('cutmix vs base acc')\n",
    "plt.ylabel('ACC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['cutmix acc', 'base acc'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MixUp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_mixup = keras.models.Sequential([\n",
    "    keras.applications.resnet.ResNet50(\n",
    "        include_top=False,\n",
    "        input_shape=(32, 32,3),\n",
    "        pooling='avg',\n",
    "    ),\n",
    "    keras.layers.Dense(num_classes, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_mixup.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=['acc']\n",
    ")\n",
    "\n",
    "resnet50_mixup_hist = resnet50_mixup.fit(\n",
    "    ds_train_mixup, # augmentation 적용하지 않은 데이터셋 사용\n",
    "    steps_per_epoch=int(ds_info.splits['train'].num_examples/BATCH_SIZE),\n",
    "    validation_steps=int(ds_info.splits['test'].num_examples/BATCH_SIZE),\n",
    "    epochs=EPOCH,\n",
    "    validation_data=ds_test,\n",
    "    verbose=1,\n",
    "    use_multiprocessing=True,\n",
    ")\n",
    "\n",
    "resnet50_mixup.save_weights('./cifar100_aug/resnet50_mixup')\n",
    "with open('./cifar100_aug/resnet50_mixup_hist', 'wb') as file_pi:\n",
    "    pickle.dump(resnet50_mixup_hist.history, file_pi)\n",
    "\n",
    "resnet50_mixup_hist = pickle.load(open('./cifar100_aug/resnet50_mixup_hist', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_mixup_hist = pickle.load(open('./cifar100_aug/resnet50_mixup_hist', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(resnet50_mixup_hist['loss'], 'r')\n",
    "plt.plot(resnet50_mixup_hist['val_loss'], 'b')\n",
    "plt.title('mixup model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train_loss', 'val_loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(resnet50_mixup_hist['categorical_accuracy'], 'r')\n",
    "plt.plot(resnet50_mixup_hist['val_categorical_accuracy'], 'b')\n",
    "plt.title('mixup model categoical_accuracy')\n",
    "plt.ylabel('ACC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train_accuracy', 'val_accuracy'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "798dbe6d6882d295d5a206f51e10fa0f74a06973d376dae7f972bab929673e71"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('aiffel')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
